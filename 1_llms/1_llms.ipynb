{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV6fHedz9b5s"
      },
      "source": [
        "# LlamaIndex Bottoms-Up Development - LLMs and Prompts\n",
        "This notebook walks through testing an LLM using the primary prompt templates used in llama-index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jbX3shl69b5u"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install llama-index\n",
        "!pip install llama-index-llms-groq\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-parse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "5NAF71hx-HnG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.groq import Groq\n",
        "\n",
        "llm = Groq(model=\"llama3-8b-8192\")\n",
        "llm_70b = Groq(model=\"llama3-70b-8192\")"
      ],
      "metadata": {
        "id": "ogp-xmIO9tMx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VZqaUnL9b5v"
      },
      "source": [
        "## Setup\n",
        "In this section, we load a test document, create an LLM, and copy prompts from llama-index to test with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xLPIUNs9b5w"
      },
      "source": [
        "First, let's load a quick document to test with. Right now, we will just load it as plain text, but we can do other operations later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e9J3uZMh9b5w"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# URL of the file to download\n",
        "url = \"https://raw.githubusercontent.com/BoxOfCereal/llama_docs_bot_groq/refs/heads/main/docs/getting_started/starter_example.md\"\n",
        "\n",
        "# Download the file and save it locally\n",
        "response = requests.get(url)\n",
        "file_path = \"starter_example.md\"\n",
        "with open(file_path, \"wb\") as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "# Open and read the downloaded file\n",
        "with open(file_path, \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# print(text)  # This will print the content of the file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OPkd_yM9b50"
      },
      "source": [
        "LlamaIndex uses some simple templates under the hood for answering queries -- mainly a `text_qa_template` for obtaining initial answers, and a `refine_template` for refining an existing answer when all the text does not fit into one LLM call.\n",
        "\n",
        "Let's copy the default templates, and test out our LLM with a few questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ehk9YIq_9b51"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Prompt\n",
        "\n",
        "text_qa_template = Prompt(\n",
        "    \"Context information is below.\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Given the context information and not prior knowledge, \"\n",
        "    \"answer the question: {query_str}\\n\"\n",
        ")\n",
        "\n",
        "refine_template = Prompt(\n",
        "    \"We have the opportunity to refine the original answer \"\n",
        "    \"(only if needed) with some more context below.\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"{context_msg}\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"Given the new context, refine the original answer to better \"\n",
        "    \"answer the question: {query_str}. \"\n",
        "    \"If the context isn't useful, output the original answer again.\\n\"\n",
        "    \"Original Answer: {existing_answer}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhyvgIAr9b53"
      },
      "source": [
        "Now, lets test a few questions!\n",
        "\n",
        "## Text QA Template Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OZwn8cCJ9b53",
        "outputId": "d41f68e6-98f5-4c70-d4aa-3a22e7c2d9ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "According to the context information, you can install LlamaIndex by following the installation steps.\n"
          ]
        }
      ],
      "source": [
        "question = \"How can I install llama-index?\"\n",
        "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
        "response = llm.complete(prompt)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9gwihbho9b53",
        "outputId": "92f3490f-f171-4aa4-db9b-f201039022a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "According to the context information, to create an index, you need to follow these steps:\n",
            "\n",
            "1. Clone the LlamaIndex repository using the command `git clone https://github.com/jerryjliu/llama_index.git`\n",
            "2. Navigate to the `examples` folder within the cloned repository using the command `cd llama_index`\n",
            "3. Navigate to the `paul_graham_essay` folder using the command `cd examples/paul_graham_essay`\n",
            "4. Create a new `.py` file and add the following code:\n",
            "```python\n",
            "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
            "\n",
            "documents = SimpleDirectoryReader('data').load_data()\n",
            "index = VectorStoreIndex.from_documents(documents)\n",
            "```\n",
            "This code builds an index over the documents in the `data` folder.\n",
            "\n",
            "Note that the `data` folder should contain the text files that you want to index. In this case, it's assumed to contain the text of Paul Graham's essay.\n"
          ]
        }
      ],
      "source": [
        "question = \"How do I create an index?\"\n",
        "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
        "response = llm.complete(prompt)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b9jwZHbi9b54",
        "outputId": "b4685d1c-e205-44a6-bba7-31e383877464",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the code snippet that creates an index:\n",
            "```python\n",
            "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
            "\n",
            "documents = SimpleDirectoryReader('data').load_data()\n",
            "index = VectorStoreIndex.from_documents(documents)\n",
            "```"
          ]
        }
      ],
      "source": [
        "question = \"How do I create an index? Write your answer using only code.\"\n",
        "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
        "response_gen = llm.stream_complete(prompt)\n",
        "for response in response_gen:\n",
        "    print(response.delta, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trhT-RPS9b54"
      },
      "source": [
        "## Refine Template Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jVpWFu1p9b54",
        "outputId": "8316a655-0ab2-4a98-ef52-47bb501832aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the refined answer:\n",
            "\n",
            "```\n",
            "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
            "\n",
            "# Load documents from the data folder\n",
            "documents = SimpleDirectoryReader('data').load_data()\n",
            "\n",
            "# Build the index\n",
            "index = VectorStoreIndex.from_documents(documents)\n",
            "\n",
            "# Persist the index to disk\n",
            "index.storage_context.persist()\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "question = \"How do I create an index? Write your answer using only code.\"\n",
        "existing_answer = \"\"\"To create an index using LlamaIndex, you need to follow these steps:\n",
        "\n",
        "1. Download the LlamaIndex repository by cloning it from GitHub.\n",
        "2. Navigate to the `examples/paul_graham_essay` folder in the cloned repository.\n",
        "3. Create a new Python file and import the necessary modules: `VectorStoreIndex` and `SimpleDirectoryReader`.\n",
        "4. Load the documents from the `data` folder using `SimpleDirectoryReader('data').load_data()`.\n",
        "5. Build the index using `VectorStoreIndex.from_documents(documents)`.\n",
        "6. To persist the index to disk, use `index.storage_context.persist()`.\n",
        "7. To reload the index from disk, use the `StorageContext` and `load_index_from_storage` functions.\n",
        "\n",
        "Note: This answer assumes that you have already installed LlamaIndex and have the necessary dependencies.\"\"\"\n",
        "prompt = refine_template.format(context_msg=text, query_str=question, existing_answer=existing_answer)\n",
        "response = llm.complete(prompt)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlGWFz6J9b56"
      },
      "source": [
        "### Chat Example\n",
        "The LLM also has a `chat` method that takes in a list of messages, to simulate a chat session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "C8J0j7C99b56",
        "outputId": "ee0291c2-6108-4e2d-c910-5387681542ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: Creating an index in Llama-Index is a straightforward process. Here's a step-by-step guide to help you get started:\n",
            "\n",
            "1. **Install Llama-Index**: First, make sure you have Llama-Index installed on your machine. You can install it using pip: `pip install llama-index`.\n",
            "2. **Create a new index**: Open a terminal or command prompt and navigate to the directory where you want to create your index. Run the following command to create a new index:\n",
            "```\n",
            "llma-index create <index_name>\n",
            "```\n",
            "Replace `<index_name>` with the name you want to give your index (e.g., \"my_index\").\n",
            "\n",
            "3. **Configure the index**: After creating the index, you'll need to configure it. You can do this by running the following command:\n",
            "```\n",
            "llma-index config <index_name>\n",
            "```\n",
            "This will open the configuration file for your index in your default text editor. You can customize the settings to suit your needs.\n",
            "\n",
            "4. **Add data to the index**: Once your index is configured, you can start adding data to it. You can do this by running the following command:\n",
            "```\n",
            "llma-index add <index_name> <data_file>\n",
            "```\n",
            "Replace `<data_file>` with the path to the file containing the data you want to add to your index.\n",
            "\n",
            "5. **Build the index**: After adding data to your index, you'll need to build it. You can do this by running the following command:\n",
            "```\n",
            "llma-index build <index_name>\n",
            "```\n",
            "This will create a searchable index of your data.\n",
            "\n",
            "That's it! You've now created an index in Llama-Index. You can use the `llma-index` command-line tool to search, update, and manage your index.\n",
            "\n",
            "If you have any more questions or need further assistance, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.base.llms.types import ChatMessage\n",
        "# https://docs.llamaindex.ai/en/stable/api_reference/prompts/#llama_index.core.prompts.ChatMessage\n",
        "\n",
        "chat_history = [\n",
        "    ChatMessage(role=\"system\", content=\"You are a helpful QA chatbot that can answer questions about llama-index.\"),\n",
        "    ChatMessage(role=\"user\", content=\"How do I create an index?\"),\n",
        "]\n",
        "\n",
        "response = llm.chat(chat_history)\n",
        "print(response.message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0hsW78N9b56"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we covered the low-level LLM API, and tested out some basic prompts with out documentation data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}